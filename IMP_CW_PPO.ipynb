{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "\n",
    "from PPO_file import PPO\n",
    "from IMP_CW_env import MESCEnv\n",
    "from utils import plot_reward_evolution, read_log_file, plot_reward_distribution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e2747",
   "metadata": {},
   "source": [
    "# **1. Training environment definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d61477",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_retailers = 2\n",
    "n_DCs = 1\n",
    "n_suppliers = 1\n",
    "supply_chain_structure = [[n_retailers] , [n_DCs], n_suppliers]\n",
    "\n",
    "env_train = MESCEnv(supply_chain_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c23777",
   "metadata": {},
   "source": [
    "# **2. PPO policy optimization**\n",
    "### **2.1. Initialization:** define environment and PPO algorithm hyperparameters\n",
    "Note that action space is handled as if it were continuous. Hence, actor's output represents the mean value of a multivariate normal random variable. Every time we want to sample an action from the actor's policy network, we will sample it from the multivariate normal distribution. For this reason, an initial standard deviation `action_std_init` is assumed, which is progressively decreased at a fixed `decay_rate` every `decay_freq` environment time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3051e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = env_train.observation_space.shape[0]\n",
    "num_actions = env_train.action_space.shape[0]\n",
    "\n",
    "num_epochs = 20 # Number of actor and critic network updates per batch of data\n",
    "max_train_steps =  50 * env_train.n_periods # Stopping criteria: number of environment time steps that PPO algorithm will be executed\n",
    "update_timestep = env_train.n_periods * 3 # How often to update actor and critic networks\n",
    "\n",
    "lr_actor = 0.00005\n",
    "lr_critic = 0.0001\n",
    "\n",
    "eps_clip = 0.2 # Value of epsilon in PPO clipping\n",
    "gamma = 0.99 # Discount factor to evaluate return\n",
    "\n",
    "action_std_init = 0.5\n",
    "decay_freq = 1000\n",
    "decay_rate = 0.03\n",
    "action_std_min = 0.05\n",
    "\n",
    "log_freq = int(env_train.n_periods/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec907b",
   "metadata": {},
   "source": [
    "### **2.2. Training**: implement PPO algorithm\n",
    "First, create a PPO agent specifying all the hyperparameters defined above. These objects come with built-in functions to facilitate the development of the coursework <span style=\"color:red\">(...)</span>.\n",
    "\n",
    "<span style=\"color:red\">**TO DO:**</span> An minimum explanation of how it works and how should they use it to implement the training function should be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ccd80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPOagent = PPO( num_states, num_actions, lr_actor, lr_critic,\n",
    "                action_std_init=action_std_init, decay_freq=decay_freq, decay_rate=decay_rate, action_std_min=action_std_min,\n",
    "                num_epochs=num_epochs, gamma=gamma, eps_clip=eps_clip, max_train_steps=max_train_steps, update_timestep=update_timestep,\n",
    "                log_freq=log_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeebbb5",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">**TO DO:**</span> In the next cell, students will be encouraged to write their own PPO implementation instead of calling the built-in train method. In fact, it could/should be removed upon deployment.\n",
    "- An additional idea is that they implement both the `train` and `update` methods. They could write the functions, and these functions will be subsequently set in the parent `PPOagent` object.\n",
    "```python\n",
    "def train(PPOagent, env, logging = True, verbose = False):\n",
    "    # Logging set up (lines of code that shouln't be changed)\n",
    "\n",
    "    # PPO algorithm (can be modified)\n",
    "\n",
    "    # Lines of code to close logging file and save model weights\n",
    "\n",
    "    return actor_model_path, log_file_path\n",
    "\n",
    "train(PPOagent, env_train, logging = True, verbose)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38231f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model_path, log_file_path = PPOagent.train(env_train, logging=True, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28c136",
   "metadata": {},
   "source": [
    "### **2.3 Visualization:** plot reward evolution during training\n",
    "If the data to be plotted is in a log file, please, indicate the appropiate file to be read in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ddd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps, rewards = read_log_file(log_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01931fed",
   "metadata": {},
   "source": [
    "Run the following cell to visualize the plot. Take into account that `timesteps` and `rewards` should be one-dimensional arrays or lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2931f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_evolution(timesteps, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64795be",
   "metadata": {},
   "source": [
    "# **3. PPO policy evaluation**\n",
    "### **3.1. Test data**: generate new demand scenarios\n",
    "It will contain 100 samples for testing, each with 52 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b31d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_demand_dataset = []\n",
    "env_train.seed = 42\n",
    "for _ in range(0,100):\n",
    "    demands_episode, _ = env_train.sample_demands_episode()\n",
    "    test_demand_dataset.append(demands_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a68005",
   "metadata": {},
   "source": [
    "### **3.2. Load policy**\n",
    "If the last trained model is not to be tested, specify the appropriate path to the actor model in the variable `actor_model_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274186a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPOagent.policy_old.actor.load_state_dict(torch.load(actor_model_path))\n",
    "PPOagent.policy.actor.load_state_dict(torch.load(actor_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b547a",
   "metadata": {},
   "source": [
    "### **3.3. Evaluation:** define the test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = PPOagent.evaluate_policy(env_train, test_demand_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136c1ad",
   "metadata": {},
   "source": [
    "Compare to heuristic policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HeuristicPolicy_file import HeuristicPolicy\n",
    "\n",
    "sSpolicy = HeuristicPolicy()\n",
    "sSpolicy.policy_param = np.array([16., 49., 15., 50., 65., 199.])\n",
    "reward_list_sS = sSpolicy.evaluate_policy(env_train, test_demand_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_reward_distribution\n",
    "plot_reward_distribution(np.vstack([reward_list,reward_list_sS]).T, labels=['PPO', 'sS policy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4ce_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
