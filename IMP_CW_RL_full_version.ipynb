{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import copy\n",
    "import torch\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from IMP_CW_env import MESCEnv\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib ipympl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e2747",
   "metadata": {},
   "source": [
    "# **1. Training environment definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d61477",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_retailers = 2\n",
    "n_DCs = 1\n",
    "n_suppliers = 1\n",
    "supply_chain_structure = [[n_retailers] , [n_DCs], n_suppliers]\n",
    "\n",
    "env_train = MESCEnv(supply_chain_structure, num_periods = 7*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afad8fc",
   "metadata": {},
   "source": [
    "# **2. Policy Network**\n",
    "[comment]: <> (MLPregresor from sklearn is not used because you cannot specify input and output shapes prior to calling the fit method. Since in the ES approach it won't be used, we build NN using Pytorch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f77cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, h1_size = 128, h2_size = 64):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, h1_size)\n",
    "        self.fc2 = torch.nn.Linear(h1_size, h2_size)\n",
    "        self.fc3 = torch.nn.Linear(h2_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a3cfc",
   "metadata": {},
   "source": [
    "# **3. Policy optimization**\n",
    "To obtain the optimal policies, different methods can be used. \n",
    "- **Heuristic methods**, like the (s,S) policy\n",
    "- **Meta-heuristic algorithms**, like simulated annealing (SA) or particle-swarm optimization (PSO).\n",
    "- **RL-specific algorithms**, such as Q-learning or REINFORCE.\n",
    "\n",
    "## 3.1 Simulated Annealing\n",
    "Simulated Annealing is a simple optimization technique inspired by the thermodynamic process of cooling metal. The algorithm iteratively explores the feasible region, accepting better solutions and, at times, worse solutions based on a probability function driven by temperature. Over time, the \"temperature\" decreases, reducing the likelihood of accepting worse solutions. \n",
    "\n",
    "You can find more information about it [here](https://www.baeldung.com/cs/simulated-annealing), but the basic flowchart is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d56a29",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2023/03/flowchart.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5db73",
   "metadata": {},
   "source": [
    "### **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Helper functions\n",
    "#################################\n",
    "\n",
    "def sample_uniform_params(params_prev, param_min, param_max):\n",
    "    '''\n",
    "    Sample random point within given parameter bounds. Tailored for EXPLORATORY purposes\n",
    "    '''\n",
    "    params = {k: torch.rand(v.shape) * (param_max - param_min) + param_min \\\n",
    "              for k, v in params_prev.items()}\n",
    "    return params\n",
    "\n",
    "def sample_local_params(params_prev, param_min, param_max):\n",
    "    '''\n",
    "    Sample a random point in the neighborhood of a given point or value or the parameters (v). Tailored for EXPLOITATION purposes\n",
    "\n",
    "    Explanation:\n",
    "    sign = (torch.randint(2, (v.shape)) * 2 - 1) # This returns either -1 or 1\n",
    "    eps = torch.rand(v.shape) * (param_max - param_min) # This returns the width of the step to be taken in the modification of the parameters\n",
    "    Hence, the total update is: v + sign*eps.\n",
    "    '''\n",
    "    params = {k: torch.rand(v.shape) * (param_max - param_min) * (torch.randint(2, (v.shape))*2 - 1) + v \\\n",
    "              for k, v in params_prev.items()}\n",
    "    return params\n",
    "\n",
    "#####################\n",
    "# Objective function \n",
    "######################\n",
    "def reward_fcn(policy_net, env, num_episodes=10, demand=None):\n",
    "    '''\n",
    "    Runs an episode and computes the total return.\n",
    "    '''\n",
    "    assert num_episodes > 0, \"Number of episodes must be greater than 0\"\n",
    "\n",
    "    env.demands_episode = demand\n",
    "    reward_list = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment before each episode\n",
    "        env.reset()\n",
    "        state = env.state\n",
    "        episode_terminated = False\n",
    "        # Initialize reward counter\n",
    "        total_reward = 0\n",
    "        \n",
    "        while episode_terminated == False:\n",
    "            # Pre-process state\n",
    "            state = torch.FloatTensor(state)\n",
    "\n",
    "            # Get action according to the policy network\n",
    "            action_mean = policy_net(state)\n",
    "            action_mean = action_mean.detach().numpy()\n",
    "            action_mean = np.floor(action_mean)\n",
    "            \n",
    "            # Interact with the environment to get reward and next state\n",
    "            state , reward, episode_terminated, _ = env.step(action_mean)\n",
    "            total_reward += reward\n",
    "           \n",
    "        reward_list.append(total_reward)\n",
    "        \n",
    "    mean_reward = np.mean(reward_list)\n",
    "    std_reward = np.std(reward_list)\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "#################################\n",
    "# 2-Phase Policy Optimization\n",
    "#################################\n",
    "def optimization_algorithm(env,\n",
    "                           param_min = -1.,\n",
    "                           param_max = 1.,\n",
    "                           num_episodes_avg = 10 ,\n",
    "                           max_iter = 100, \n",
    "                           ratio_rs_ls = 0.1, \n",
    "                           NNparams_0 = None,\n",
    "                           initial_temp = 1e5, \n",
    "                           max_time = 30, # maximum execution time in seconds\n",
    "                           ):\n",
    "    \n",
    "    assert initial_temp > 0 , \"inital_temp must be a positive value\"\n",
    "\n",
    "    # Setup iterations\n",
    "    iter_rs = round(max_iter * ratio_rs_ls)\n",
    "    iter_ls = max_iter - iter_rs\n",
    "\n",
    "    # Initialize buffers to store data for plotting\n",
    "    reward_history = []\n",
    "    std_history = []\n",
    "    best_reward_history = []\n",
    "\n",
    "    # Policy initialization\n",
    "    policy_net = PolicyNetwork(input_size=env.observation_space.shape[0], \n",
    "                               output_size=env.action_space.shape[0])\n",
    "    param = policy_net.state_dict() if NNparams_0 is None else NNparams_0 \n",
    "\n",
    "    # Initialization\n",
    "    best_param = copy.deepcopy(param)\n",
    "    best_reward, best_std = reward_fcn(policy_net, env, num_episodes=num_episodes_avg)\n",
    "    reward_history.append(best_reward)\n",
    "    std_history.append(best_std)\n",
    "    best_reward_history.append(best_reward)\n",
    "    i = 0\n",
    "\n",
    "    # Define path to store best policies\n",
    "    save_path = setup_model_saving(algorithm=\"SA\")\n",
    "    print(f\"Policy model weights saved in: {save_path}\")\n",
    "\n",
    "    # Initialize timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # # ###########################################################   \n",
    "    # # Random search phase: generate good initial solution\n",
    "    # # ###########################################################\n",
    "    if NNparams_0 is None:\n",
    "        while (i < iter_rs) and (i < max_iter) and ((time.time()-start_time) < max_time):\n",
    "            # Update iteration counter\n",
    "            i += 1\n",
    "\n",
    "            # Sample a random policy\n",
    "            current_param = sample_uniform_params(param, param_min, param_max)\n",
    "            # Construct the policy network with the sampled parameters\n",
    "            policy_net.load_state_dict(current_param)\n",
    "            # Evaluate the policy\n",
    "            reward, std = reward_fcn(policy_net, env, num_episodes=num_episodes_avg)\n",
    "            reward_history.append(reward)\n",
    "            std_history.append(std)\n",
    "\n",
    "            if not np.isfinite(reward):\n",
    "                print(f\"Reward overflow: {reward}\")\n",
    "\n",
    "            # Check if the new policy is better than the best one found so far\n",
    "            if reward > best_reward:\n",
    "                best_reward = reward\n",
    "                best_param = copy.deepcopy(param)\n",
    "\n",
    "            # Store the best reward found so far for plotting\n",
    "            best_reward_history.append(best_reward)\n",
    "\n",
    "    # ###########################################################\n",
    "    # Local search phase: Simulated Annealing\n",
    "    # ###########################################################\n",
    "\n",
    "    # Set current working solution\n",
    "    current_param = copy.deepcopy(best_param)\n",
    "    current_reward = best_reward\n",
    "    # Reset iteration counter\n",
    "    i = 0\n",
    "\n",
    "    while i < (max_iter - iter_rs) and ((time.time()-start_time) < max_time):\n",
    "        # Update iteration counter\n",
    "        i += 1\n",
    "        \n",
    "        # Sample a new policy in the neighborhood of the current one\n",
    "        # TODO: check if ok or leave it like sample_uniform_params\n",
    "        candidate_param = sample_local_params(current_param, param_min, param_max)\n",
    "\n",
    "        # Evaluate the candidate policy\n",
    "        policy_net.load_state_dict(candidate_param)\n",
    "        candidate_reward, std = reward_fcn(policy_net, env, num_episodes=num_episodes_avg)\n",
    "\n",
    "        # Check if the candidate policy is better than the current one\n",
    "        if candidate_reward > best_reward:\n",
    "            # Update the new best policy\n",
    "            best_reward = candidate_reward\n",
    "            best_param = copy.deepcopy(candidate_param)\n",
    "            # Save policy\n",
    "            torch.save(policy_net.state_dict(), save_path)\n",
    "            \n",
    "        # Check if the candidate policy should be kept or discarded\n",
    "        diff = candidate_reward - current_reward\n",
    "        temp = initial_temp / (1 + i) # update temperature paramter\n",
    "        metropolis = np.exp(diff/temp) # compute metropolis acceptance probability\n",
    "        if diff > 0 or np.random.rand() < metropolis:\n",
    "            # Update the current policy \n",
    "            current_param = copy.deepcopy(candidate_param)\n",
    "            current_reward = candidate_reward\n",
    "        \n",
    "        # Store the data for plotting\n",
    "        reward_history.append(candidate_reward)\n",
    "        std_history.append(std)\n",
    "        best_reward_history.append(best_reward)\n",
    "\n",
    "    if time.time() - start_time > max_time:\n",
    "        print(\"Timeout reached: the best policy found so far will be returned.\")\n",
    "\n",
    "    # Pack data for plotting\n",
    "    plot_info = {'reward_history': reward_history,\n",
    "                'std_history': std_history,\n",
    "                'best_reward_history': best_reward_history}\n",
    "            \n",
    "    return best_param, best_reward, plot_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4bdef",
   "metadata": {},
   "source": [
    "### **Policy training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamSA = {}\n",
    "# Problem specs \n",
    "hyperparamSA['param_min'] = -1.\n",
    "hyperparamSA['param_max'] = 1.\n",
    "hyperparamSA['num_episodes_avg'] = 5\n",
    "# Algorithm-specific hyperparameters\n",
    "hyperparamSA['initial_temp'] = 1e7\n",
    "hyperparamSA['max_iter'] = 50\n",
    "hyperparamSA['ratio_rs_ls'] = .05\n",
    "# Execution control\n",
    "hyperparamSA[\"max_time\"] = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95fd80",
   "metadata": {},
   "source": [
    "By running the cell below, your training algorithm will be executed using the hyperparameter configuration you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(over='ignore')  # Suppress overflow warnings\n",
    "# np.seterr(over='warn')  # Re-enable overflow warnings\n",
    "\n",
    "best_paramSA, best_reward, plot_info = optimization_algorithm(env_train, **hyperparamSA)\n",
    "print(f\"Best reward: {best_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07280a2",
   "metadata": {},
   "source": [
    "### **Training analysis:** plot reward evolution during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87753412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solution evolution per iteration\n",
    "plot_reward_evolution(list(range(len(plot_info['reward_history']))), plot_info['reward_history'], reward_std=plot_info['std_history']);\n",
    "\n",
    "# Plot historical best reward\n",
    "plt.plot(list(range(len(plot_info['best_reward_history']))), plot_info['best_reward_history'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Historical best reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd90c1c",
   "metadata": {},
   "source": [
    "# **4. REINFORCE with baseline**\n",
    "\n",
    "REINFORCE is a foundational RL algorithm that belongs to the familiy of policy-based methods. REINFORCE updates the probability of taking specific actions in certain states based on how much those actions contributed to higher overall rewards in past episodes. The policy being learned is the same policy used to generate the data the agent uses to learn, hence \"on-policy\".\n",
    "\n",
    "<div>\n",
    "<img src=\"https://lcalem.github.io/imgs/sutton/reinforce_baseline.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b4500",
   "metadata": {},
   "source": [
    "### **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8319ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Helper functions\n",
    "#################################\n",
    "class ValueNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def choose_action(state, policy_net, action_std):\n",
    "    '''\n",
    "    Sample action in continuous action space modelled with a Multivariate Normal distribution\n",
    "    '''\n",
    "    # Predict action mean from Policy Network\n",
    "    action_mean = policy_net(torch.from_numpy(state).float())\n",
    "\n",
    "    # Estimate action variance (decaying action std)\n",
    "    action_var = torch.full(size=(policy_net.fc3.out_features,) , fill_value = action_std**2)\n",
    "    cov_mat = torch.diag(action_var).unsqueeze(dim=0) \n",
    "\n",
    "    # Generate Multivariate Normal distribution with estimated mean and variance\n",
    "    dist = torch.distributions.MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "    # Sample action\n",
    "    action = dist.sample()\n",
    "\n",
    "    # Compute logprob and entropy\n",
    "    logprob = dist.log_prob(action)\n",
    "    entropy = dist.entropy()\n",
    "\n",
    "    return action, logprob , entropy\n",
    "\n",
    "###########################################\n",
    "# REINFORCE with baseline implementation\n",
    "###########################################\n",
    "def trainREINFORCE(env, *, \n",
    "          h1_size = 128, \n",
    "          h2_size = 64,\n",
    "          lr_policy_net= 5e-5, \n",
    "          lr_value_net = 1e-5, \n",
    "          discount_factor = .99 , \n",
    "          max_steps = 1e5, \n",
    "          max_time = 120. ,\n",
    "          weight_entropy = 0.001, \n",
    "          action_std_init = .5):\n",
    "    \n",
    "    # Create log file\n",
    "    log_f_path = setup_logging(algorithm = \"REINFORCE\")\n",
    "    log_f = open(log_f_path, 'w+')\n",
    "    log_f.write(\"Timestep,Reward\\n\")\n",
    "    # Create file to store model weigths\n",
    "    save_f_path = setup_model_saving(algorithm = \"REINFORCE\")\n",
    "\n",
    "    # Initialize variables\n",
    "    counter_timesteps = 0\n",
    "    best_reward = -np.inf\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize policies and optimizers\n",
    "    policy_net = PolicyNetwork( input_size=env.observation_space.shape[0], output_size=env.action_space.shape[0],\n",
    "                                h1_size = h1_size,\n",
    "                                h2_size = h2_size)\n",
    "    value_net = ValueNetwork(input_size=env.observation_space.shape[0])\n",
    "    optimizer_policy = torch.optim.Adam(policy_net.parameters(), lr=lr_policy_net)\n",
    "    optimizer_value = torch.optim.Adam(value_net.parameters(), lr=lr_value_net)\n",
    "\n",
    "    while (counter_timesteps < max_steps) and ((time.time()-start_time) < max_time):\n",
    "        \n",
    "        # Run an episode and collect experience\n",
    "        trajectory = {}\n",
    "        trajectory[\"values\"] = []\n",
    "        trajectory[\"actions\"] = []\n",
    "        trajectory[\"logprobs\"] = []\n",
    "        trajectory[\"rewards\"] = []\n",
    "        trajectory[\"entropies\"] = []\n",
    "\n",
    "        done = False\n",
    "        env.reset()\n",
    "        state = env.state\n",
    "        \n",
    "        while not done:\n",
    "            action , action_logprob , entropy = choose_action(state, policy_net, action_std_init)\n",
    "            next_state , reward , done , _ = env.step(action.detach().numpy().flatten())\n",
    "\n",
    "            trajectory[\"values\"].append(value_net(torch.from_numpy(state).float()))\n",
    "            trajectory[\"logprobs\"].append(action_logprob)\n",
    "            trajectory[\"rewards\"].append(reward)\n",
    "            trajectory[\"entropies\"].append(entropy)\n",
    "\n",
    "            state = next_state\n",
    "            counter_timesteps += 1\n",
    "        \n",
    "        logprobs = torch.stack(trajectory[\"logprobs\"]).squeeze() # shape : (episode_length, )\n",
    "        entropies = torch.stack(trajectory[\"entropies\"]).squeeze() # shape : (episode_length, )\n",
    "        values = torch.stack(trajectory[\"values\"]).squeeze() # shape : (episode_length, )\n",
    "\n",
    "        # Calculate discounted return at every time step\n",
    "        discounted_return = 0\n",
    "        returns = []\n",
    "        for r in reversed(trajectory[\"rewards\"]):\n",
    "            discounted_return = r + discount_factor * discounted_return\n",
    "            returns.insert(0, discounted_return)\n",
    "        returns = torch.tensor(returns , dtype=torch.float32)\n",
    "\n",
    "        # Compute policy loss\n",
    "        advantages = returns - values\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        loss_policy = (-1) * torch.mean(advantages.detach() * logprobs) + weight_entropy * ((-1) * torch.mean(entropies))\n",
    "\n",
    "        # Compute value loss\n",
    "        loss_value = torch.nn.functional.mse_loss(values , returns)\n",
    "\n",
    "        # Update policy network\n",
    "        optimizer_policy.zero_grad()\n",
    "        loss_policy.backward()\n",
    "        optimizer_policy.step()\n",
    "\n",
    "        # Update Value Network\n",
    "        optimizer_value.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer_value.step()\n",
    "\n",
    "        # Write episode undiscounted return to log file\n",
    "        total_return = round(np.mean(sum(trajectory[\"rewards\"])), 4)\n",
    "        log_f.write('{:.0f},{:.3f}\\n'.format(counter_timesteps, total_return))\n",
    "        log_f.flush() \n",
    "\n",
    "        # Save best policy\n",
    "        if total_return > best_reward:\n",
    "            best_reward = total_return\n",
    "        torch.save(policy_net.state_dict(), save_f_path)\n",
    "\n",
    "    # Close log file\n",
    "    log_f.close()\n",
    "    \n",
    "    print(f\"Log file saved in: {log_f_path}\") \n",
    "    print(f\"Policy model weights saved in: {save_f_path}\") \n",
    "    print(f\"Best reward: {best_reward}\")\n",
    "\n",
    "    return save_f_path , log_f_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219178b",
   "metadata": {},
   "source": [
    "### **Hyperparamter definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ab2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamRL = {}\n",
    "hyperparamRL[\"max_steps\"] = env_train.n_periods * 500 # env_train.n_periods returns the episode length\n",
    "hyperparamRL[\"weight_entropy\"] = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ccc8e",
   "metadata": {},
   "source": [
    "### **Policy training** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aff567",
   "metadata": {},
   "outputs": [],
   "source": [
    "policyRL_model_path , log_file_path = trainREINFORCE(env_train, **hyperparamRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a5224",
   "metadata": {},
   "source": [
    "### **Training analysis:** plot reward evolution during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a866df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps, rewards = read_log_file(log_file_path)\n",
    "plot_reward_evolution(timesteps, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64795be",
   "metadata": {},
   "source": [
    "# **5. Policy evaluation**\n",
    "### **5.1. Test data**: get new demand scenarios\n",
    "The variable test_demand_dataset contains 100 samples of demand scenarios, each with 52 weeks, that will be used for testing and comparing the extrapolation capabilities of the different policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f853ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in binary mode\n",
    "with open(\"test_demand_dataset.pickle\", 'rb') as file:\n",
    "    # Deserialize and retrieve the variable from the file\n",
    "    test_demand_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b547a",
   "metadata": {},
   "source": [
    "### **5.2. Evaluate optimal policies**\n",
    "To assess the performance of each algorithm, the optimal policies are evaluated on the test dataset to compute the average undiscounted return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b76a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Evaluation function\n",
    "#######################################################\n",
    "def calculate_average_return(policy , env , demand):\n",
    "    '''\n",
    "    Runs an episode and computes the total return.\n",
    "    '''\n",
    "    reward_list = []\n",
    "\n",
    "    for i in range(len(demand)):\n",
    "        # Set the demand of each episode\n",
    "        env.demand_dataset = demand[i]\n",
    "        # Reset environment before each episode\n",
    "        env.reset()\n",
    "        state = env.state\n",
    "        episode_terminated = False\n",
    "        # Initialize reward counter\n",
    "        total_reward = 0\n",
    "        \n",
    "        while episode_terminated == False:\n",
    "            # Pre-process state\n",
    "            state = torch.FloatTensor(state)\n",
    "\n",
    "            # Get action according to the policy network\n",
    "            action_mean = policy(state)\n",
    "            action_mean = action_mean.detach().numpy()\n",
    "            action_mean = np.floor(action_mean)\n",
    "            \n",
    "            # Interact with the environment to get reward and next state\n",
    "            state , reward, episode_terminated, _ = env.step(action_mean)\n",
    "            total_reward += reward\n",
    "\n",
    "        reward_list.append(total_reward)\n",
    "    \n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load policy derived using Simulated Annealing\n",
    "policySA = PolicyNetwork(input_size=env_train.observation_space.shape[0], output_size=env_train.action_space.shape[0]) \n",
    "policySA.load_state_dict(best_paramSA)\n",
    "\n",
    "# Compute average undiscounted return\n",
    "reward_list[\"SA\"] = []\n",
    "rewardSA = calculate_average_return(policySA, env_train, test_demand_dataset)\n",
    "reward_list[\"SA\"].append(rewardSA)\n",
    "\n",
    "# Print results\n",
    "print(\"Performance of SA:\\n - Average reward: {:.2E}\\n - Reward standard deviation: {:.2E}\".format(np.mean(reward_list[\"SA\"]), np.std(reward_list[\"SA\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de78a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load policy derived using REINFORCE with baseline\n",
    "policyRL = PolicyNetwork(input_size=env_train.observation_space.shape[0], output_size=env_train.action_space.shape[0],)\n",
    "policyRL.load_state_dict(torch.load(policyRL_model_path))\n",
    "\n",
    "# Compute average undiscounted return\n",
    "reward_list[\"RL\"] = []\n",
    "rewardRL = calculate_average_return(policyRL, env_train, test_demand_dataset)\n",
    "reward_list[\"RL\"].append(rewardRL)\n",
    "\n",
    "# Print results\n",
    "print(\"Performance of RL:\\n - Average reward: {:.2E}\\n - Reward standard deviation: {:.2E}\".format(np.mean(reward_list[\"RL\"]), np.std(reward_list[\"RL\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136c1ad",
   "metadata": {},
   "source": [
    "Evaluate a pre-computed optimal heuristic policy for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HeuristicPolicy_file import HeuristicPolicy\n",
    "\n",
    "sSpolicy = HeuristicPolicy()\n",
    "sSpolicy.policy_param = np.array([19., 34., 17., 25., 79., 138.])\n",
    "reward_list[\"sS\"] = sSpolicy.evaluate_policy(env_train, test_demand_dataset)\n",
    "print(\"Heuristic policy performance:\\n - Average reward: {:.2E}\\n - Reward standard deviation: {:.2E}\".format(np.mean(reward_list[\"sS\"]), np.std(reward_list[\"sS\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04fd73",
   "metadata": {},
   "source": [
    "### **5.3 Result analysis:** reward distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_distribution(np.vstack([reward_list[\"SA\"],reward_list[\"REINFORCE\"],reward_list[\"sS\"]]).T, labels=['SA', 'REINFORCE', 'Heuristic policy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4ce_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
