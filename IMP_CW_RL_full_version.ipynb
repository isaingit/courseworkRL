{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4733c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import copy\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from IMP_CW_env import MESCEnv\n",
    "from utils import plot_reward_evolution, read_log_file, plot_reward_distribution, setup_model_saving\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib ipympl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e2747",
   "metadata": {},
   "source": [
    "# **1. Training environment definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d61477",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_retailers = 2\n",
    "n_DCs = 1\n",
    "n_suppliers = 1\n",
    "supply_chain_structure = [[n_retailers] , [n_DCs], n_suppliers]\n",
    "\n",
    "env_train = MESCEnv(supply_chain_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afad8fc",
   "metadata": {},
   "source": [
    "# **2. Policy Network**\n",
    "[comment]: <> (MLPregresor from sklearn is not used because you cannot specify input and output shapes prior to calling the fit method. Since in the ES approach it won't be used, we build NN using Pytorch.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f77cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, h1_size = 128, h2_size = 64):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, h1_size)\n",
    "        self.fc2 = torch.nn.Linear(h1_size, h2_size)\n",
    "        self.fc3 = torch.nn.Linear(h2_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a3cfc",
   "metadata": {},
   "source": [
    "# **3. Policy optimization**\n",
    "To obtain the optimal policies, different methods can be used. \n",
    "- **Heuristic methods**, like the (s,S) policy\n",
    "- **Meta-heuristic algorithms**, like simulated annealing (SA) or particle-swarm optimization (PSO).\n",
    "- **RL-specific algorithms**, such as Q-learning or REINFORCE.\n",
    "\n",
    "## 3.1 Simulated Annealing\n",
    "Simulated Annealing is a simple optimization technique inspired by the thermodynamic process of cooling metal. The algorithm iteratively explores the feasible region, accepting better solutions and, at times, worse solutions based on a probability function driven by temperature. Over time, the \"temperature\" decreases, reducing the likelihood of accepting worse solutions. \n",
    "\n",
    "You can find more information about it [here](https://www.baeldung.com/cs/simulated-annealing), but the basic flowchart is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d56a29",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2023/03/flowchart.png\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5db73",
   "metadata": {},
   "source": [
    "### **Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Helper functions\n",
    "#################################\n",
    "\n",
    "def sample_uniform_params(params_prev, param_min, param_max):\n",
    "    '''\n",
    "    Sample random point within given parameter bounds. Tailored for EXPLORATORY purposes\n",
    "    '''\n",
    "    params = {k: torch.rand(v.shape) * (param_max - param_min) + param_min \\\n",
    "              for k, v in params_prev.items()}\n",
    "    return params\n",
    "\n",
    "def sample_local_params(params_prev, param_min, param_max):\n",
    "    '''\n",
    "    Sample a random point in the neighborhood of a given point or value or the parameters (v). Tailored for EXPLOITATION purposes\n",
    "\n",
    "    Explanation:\n",
    "    sign = (torch.randint(2, (v.shape)) * 2 - 1) # This returns either -1 or 1\n",
    "    eps = torch.rand(v.shape) * (param_max - param_min) # This returns the width of the step to be taken in the modification of the parameters\n",
    "    Hence, the total update is: v + sign*eps.\n",
    "    '''\n",
    "    params = {k: torch.rand(v.shape) * (param_max - param_min) * (torch.randint(2, (v.shape))*2 - 1) + v \\\n",
    "              for k, v in params_prev.items()}\n",
    "    return params\n",
    "\n",
    "#####################\n",
    "# Objective function \n",
    "######################\n",
    "def reward_fcn(policy_net, env, num_episodes=10, demand=None):\n",
    "    '''\n",
    "    Runs an episode and computes the total return.\n",
    "    '''\n",
    "    assert num_episodes > 0, \"Number of episodes must be greater than 0\"\n",
    "\n",
    "    env.demands_episode = demand\n",
    "    reward_list = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        # Reset environment before each episode\n",
    "        env.reset()\n",
    "        state = env.state\n",
    "        episode_terminated = False\n",
    "        # Initialize reward counter\n",
    "        total_reward = 0\n",
    "        \n",
    "        while episode_terminated == False:\n",
    "            # Pre-process state\n",
    "            state = torch.FloatTensor(state)\n",
    "\n",
    "            # Get action according to the policy network\n",
    "            action_mean = policy_net(state)\n",
    "            action_mean = action_mean.detach().numpy()\n",
    "            action_mean = np.floor(action_mean)\n",
    "            \n",
    "            # Interact with the environment to get reward and next state\n",
    "            state , reward, episode_terminated, _ = env.step(action_mean)\n",
    "            total_reward += reward\n",
    "           \n",
    "        reward_list.append(total_reward)\n",
    "\n",
    "    mean_reward = np.mean(reward_list)\n",
    "    std_reward = np.std(reward_list)\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "#################################\n",
    "# 2-Phase Policy Optimization\n",
    "#################################\n",
    "def optimization_algorithm(env,\n",
    "                           param_min = -1.,\n",
    "                           param_max = 1.,\n",
    "                           num_episodes_avg = 10 ,\n",
    "                           max_iter = 100, \n",
    "                           ratio_rs_ls = 0.1, \n",
    "                           NNparams_0 = None,\n",
    "                           initial_temp = 1e5, \n",
    "                           max_time = 30, # maximum execution time in seconds\n",
    "                           ):\n",
    "    \n",
    "    assert initial_temp > 0 , \"inital_temp must be a positive value\"\n",
    "\n",
    "    # Setup iterations\n",
    "    iter_rs = round(max_iter * ratio_rs_ls)\n",
    "    iter_ls = max_iter - iter_rs\n",
    "\n",
    "    # Initialize buffers to store data for plotting\n",
    "    reward_history = []\n",
    "    std_history = []\n",
    "    best_reward_history = []\n",
    "\n",
    "    # Policy initialization\n",
    "    policy_net = PolicyNetwork(input_size=env.observation_space.shape[0], \n",
    "                               output_size=env.action_space.shape[0])\n",
    "    param = policy_net.state_dict() if NNparams_0 is None else NNparams_0 \n",
    "\n",
    "    # Initialization\n",
    "    best_param = copy.deepcopy(param)\n",
    "    best_reward, best_std = reward_fcn(policy_net, env, num_episodes=num_episodes_avg)\n",
    "    reward_history.append(best_reward)\n",
    "    std_history.append(best_std)\n",
    "    best_reward_history.append(best_reward)\n",
    "    i = 0\n",
    "\n",
    "    # Define path to store best policies\n",
    "    save_path = setup_model_saving(algorithm=\"SA\")\n",
    "    print(f\"Policy model weights saved in: {save_path}\")\n",
    "\n",
    "    # Initialize timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # # ###########################################################   \n",
    "    # # Random search phase: generate good initial solution\n",
    "    # # ###########################################################\n",
    "    if NNparams_0 is None:\n",
    "        while (i < iter_rs) and (i < max_iter) and ((time.time()-start_time) < max_time):\n",
    "            # Update iteration counter\n",
    "            i += 1\n",
    "\n",
    "            # Sample a random policy\n",
    "            current_param = sample_uniform_params(param, param_min, param_max)\n",
    "            # Construct the policy network with the sampled parameters\n",
    "            policy_net.load_state_dict(current_param)\n",
    "            # Evaluate the policy\n",
    "            reward, std = reward_fcn(policy_net, env, num_episodes=num_episodes_avg)\n",
    "            reward_history.append(reward)\n",
    "            std_history.append(std)\n",
    "\n",
    "            if not np.isfinite(reward):\n",
    "                print(f\"Reward overflow: {reward}\")\n",
    "\n",
    "            # Check if the new policy is better than the best one found so far\n",
    "            if reward > best_reward:\n",
    "                best_reward = reward\n",
    "                best_param = copy.deepcopy(param)\n",
    "\n",
    "            # Store the best reward found so far for plotting\n",
    "            best_reward_history.append(best_reward)\n",
    "\n",
    "    # ###########################################################\n",
    "    # Local search phase: Simulated Annealing\n",
    "    # ###########################################################\n",
    "\n",
    "    # Set current working solution\n",
    "    current_param = copy.deepcopy(best_param)\n",
    "    current_reward = best_reward\n",
    "    # Reset iteration counter\n",
    "    i = 0\n",
    "\n",
    "    while i < (max_iter - iter_rs) and ((time.time()-start_time) < max_time):\n",
    "        # Update iteration counter\n",
    "        i += 1\n",
    "        \n",
    "        # Sample a new policy in the neighborhood of the current one\n",
    "        # TODO: check if ok or leave it like sample_uniform_params\n",
    "        candidate_param = sample_local_params(current_param, param_min, param_max)\n",
    "\n",
    "        # Evaluate the candidate policy\n",
    "        policy_net.load_state_dict(candidate_param)\n",
    "        candidate_reward, std = reward_fcn(policy_net, env, num_episodes=num_episodes_avg)\n",
    "\n",
    "        # Check if the candidate policy is better than the current one\n",
    "        if candidate_reward > best_reward:\n",
    "            # Update the new best policy\n",
    "            best_reward = candidate_reward\n",
    "            best_param = copy.deepcopy(candidate_param)\n",
    "            # Save policy\n",
    "            torch.save(policy_net.state_dict(), save_path)\n",
    "            \n",
    "        # Check if the candidate policy should be kept or discarded\n",
    "        diff = candidate_reward - current_reward\n",
    "        temp = initial_temp / (1 + i) # update temperature paramter\n",
    "        metropolis = np.exp(diff/temp) # compute metropolis acceptance probability\n",
    "        if diff > 0 or np.random.rand() < metropolis:\n",
    "            # Update the current policy \n",
    "            current_param = copy.deepcopy(candidate_param)\n",
    "            current_reward = candidate_reward\n",
    "        \n",
    "        # Store the data for plotting\n",
    "        reward_history.append(candidate_reward)\n",
    "        std_history.append(std)\n",
    "        best_reward_history.append(best_reward)\n",
    "\n",
    "    if time.time() - start_time > max_time:\n",
    "        print(\"Timeout reached: the best policy found so far will be returned.\")\n",
    "\n",
    "    # Pack data for plotting\n",
    "    plot_info = {'reward_history': reward_history,\n",
    "                'std_history': std_history,\n",
    "                'best_reward_history': best_reward_history}\n",
    "            \n",
    "    return best_param, best_reward, plot_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4bdef",
   "metadata": {},
   "source": [
    "### **Policy training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6856d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamSA = {}\n",
    "# Problem specs \n",
    "hyperparamSA['param_min'] = -1.\n",
    "hyperparamSA['param_max'] = 1.\n",
    "hyperparamSA['num_episodes_avg'] = 5\n",
    "# Algorithm-specific hyperparameters\n",
    "hyperparamSA['initial_temp'] = 1e7\n",
    "hyperparamSA['max_iter'] = 50\n",
    "hyperparamSA['ratio_rs_ls'] = .05\n",
    "# Execution control\n",
    "hyperparamSA[\"max_time\"] = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f95fd80",
   "metadata": {},
   "source": [
    "By running the cell below, your training algorithm will be executed using the hyperparameter configuration you defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(over='ignore')  # Suppress overflow warnings\n",
    "# np.seterr(over='warn')  # Re-enable overflow warnings\n",
    "\n",
    "best_paramSA, best_reward, plot_info = optimization_algorithm(env_train, **hyperparamSA)\n",
    "print(f\"Best reward: {best_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07280a2",
   "metadata": {},
   "source": [
    "Visualize training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87753412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solution evolution per iteration\n",
    "plot_reward_evolution(list(range(len(plot_info['reward_history']))), plot_info['reward_history'], reward_std=plot_info['std_history']);\n",
    "\n",
    "# Plot historical best reward\n",
    "plt.plot(list(range(len(plot_info['best_reward_history']))), plot_info['best_reward_history'])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Historical best reward');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd90c1c",
   "metadata": {},
   "source": [
    "# **4. REINFORCE**\n",
    "\n",
    "REINFORCE is a foundational RL algorithm that belongs to the familiy of policy-based methods. REINFORCE updates the probability of taking specific actions in certain states based on how much those actions contributed to higher overall rewards in past episodes. The policy being learned is the same policy used to generate the data the agent uses to learn, hence \"on-policy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea971ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamRL = {}\n",
    "hyperparamRL[\"num_trajectories\"]= 3 # Number of trajectories used to approximate the expected return\n",
    "hyperparamRL[\"max_timesteps\"] =  3 * 25 * env_train.n_periods # Stopping criteria: number of environment time steps that PPO algorithm will be executed\n",
    "hyperparamRL[\"discount_factor\"] = 0.99 # Discount factor to evaluate return\n",
    "\n",
    "hyperparamRL[\"action_std_init\"] = .5\n",
    "hyperparamRL[\"decay_freq\"] = 1000\n",
    "hyperparamRL[\"decay_rate\"] = 0.03\n",
    "hyperparamRL[\"action_std_min\"] = 0.05\n",
    "\n",
    "hyperparamRL[\"h1_size\"] = 128\n",
    "hyperparamRL[\"h2_size\"] = 64\n",
    "\n",
    "hyperparamRL[\"lr_policy_net\"] = 0.00005\n",
    "hyperparamRL[\"lr_value_net\"] = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8319ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from REINFORCE_file import train_REINFORCE\n",
    "\n",
    "optimal_policyRL , policyRL_model_path = train_REINFORCE(env_train, **hyperparamRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49a5224",
   "metadata": {},
   "source": [
    "### **Training analysis:** plot reward evolution during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a866df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = r\"REINFORCE_logs//REINFORCE_log_2.csv\"\n",
    "timesteps, rewards, std = read_log_file(log_file_path)\n",
    "plot_reward_evolution(timesteps, rewards, reward_std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64795be",
   "metadata": {},
   "source": [
    "# **5. Policy evaluation**\n",
    "### **5.1. Test data**: get new demand scenarios\n",
    "The variable test_demand_dataset contains 100 samples of demand scenarios, each with 52 weeks, that will be used for testing and comparing the extrapolation capabilities of the different policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f853ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Open the file in binary mode\n",
    "with open(\"test_demand_dataset.pickle\", 'rb') as file:\n",
    "    # Deserialize and retrieve the variable from the file\n",
    "    test_demand_dataset = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a68005",
   "metadata": {},
   "source": [
    "### **5.2. Load optimal policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274186a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "policySA = PolicyNetwork(input_size=env_train.observation_space.shape[0], output_size=env_train.action_space.shape[0]) \n",
    "policySA.load_state_dict(best_paramSA)\n",
    "\n",
    "policyRL = PolicyNetwork(input_size=env_train.observation_space.shape[0], output_size=env_train.action_space.shape[0],\n",
    "                         h1_size=hyperparamRL[\"h1_size\"] , h2_size=hyperparamRL[\"h2_size\"])\n",
    "policyRL.load_state_dict(torch.load(policyRL_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b547a",
   "metadata": {},
   "source": [
    "### **5.3. Evaluation**\n",
    "First, define and execute the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dbf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = {}\n",
    "reward_list[\"SA\"] = []\n",
    "reward_list[\"REINFORCE\"] = []\n",
    "reward_list[\"sS\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for demand in test_demand_dataset:\n",
    "    rewardSA, _ = reward_fcn(policySA, env_train, num_episodes=1, demand=demand)\n",
    "    reward_list[\"SA\"].append(rewardSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de78a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for demand in test_demand_dataset:\n",
    "    rewardRL, _ = reward_fcn(policyRL, env_train, num_episodes=1, demand=demand)\n",
    "    reward_list[\"REINFORCE\"].append(rewardRL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bac350",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance of SA:\\n - Average reward: {:.0f}\\n - Reward standard deviation: {:.2f}\".format(np.mean(reward_list[\"SA\"]), np.std(reward_list[\"SA\"])))\n",
    "print(\"Performance of REINFORCE:\\n - Average reward: {:.0f}\\n - Reward standard deviation: {:.2f}\".format(np.mean(reward_list[\"REINFORCE\"]), np.std(reward_list[\"REINFORCE\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136c1ad",
   "metadata": {},
   "source": [
    "Compare to heuristic policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76cb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HeuristicPolicy_file import HeuristicPolicy\n",
    "\n",
    "sSpolicy = HeuristicPolicy()\n",
    "sSpolicy.policy_param = np.array([19., 34., 17., 25., 79., 138.])\n",
    "reward_list[\"sS\"] = sSpolicy.evaluate_policy(env_train, test_demand_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f909b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Heuristic policy performance:\\n - Average reward: {:.0f}\\n - Reward standard deviation: {:.2f}\".format(np.mean(reward_list[\"sS\"]), np.std(reward_list[\"sS\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04fd73",
   "metadata": {},
   "source": [
    "Visualize reward distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_reward_distribution\n",
    "plot_reward_distribution(np.vstack([reward_list[\"SA\"],reward_list[\"REINFORCE\"],reward_list[\"sS\"]]).T, labels=['SA', 'REINFORCE', 'Heuristic policy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4ce_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
